architecture: transformer
d_model: 512
num_heads: 8
num_encoder_layers: 12
num_decoder_layers: 6
dropout: 0.3
ffnet_style: ff
teacher_forcing_ratio: 1.0
teacher_forcing_step: 0.0
min_teacher_forcing_ratio: 1.0
joint_ctc_attention: false
max_len: 400