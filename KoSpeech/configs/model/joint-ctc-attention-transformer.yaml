architecture: transformer
d_model: 512
num_heads: 8
num_encoder_layers: 12
num_decoder_layers: 6
dropout: 0.3
ffnet_style: ff
teacher_forcing_ratio: 1.0
teacher_forcing_step: 0.0
min_teacher_forcing_ratio: 1.0
cross_entropy_weight: 0.7
ctc_weight: 0.3
mask_conv: True
joint_ctc_attention: True
max_len: 400