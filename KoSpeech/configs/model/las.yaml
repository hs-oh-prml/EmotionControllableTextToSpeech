architecture: las
use_bidirectional: True
hidden_dim: 512
dropout: 0.3
num_heads: 4
label_smoothing: 0.1
num_encoder_layers: 3
num_decoder_layers: 2
rnn_type: lstm
teacher_forcing_ratio: 1.0
attn_mechanism: multi-head
teacher_forcing_step: 0.0
min_teacher_forcing_ratio: 1.0
extractor: vgg
activation: hardtanh
mask_conv: false
joint_ctc_attention: false
max_len: 400