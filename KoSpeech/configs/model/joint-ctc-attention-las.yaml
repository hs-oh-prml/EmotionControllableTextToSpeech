architecture: las
use_bidirectional: True
hidden_dim: 768
dropout: 0.3
num_heads: 4
label_smoothing: 0.1
num_encoder_layers: 3
num_decoder_layers: 2
rnn_type: lstm
teacher_forcing_ratio: 1.0
attn_mechanism: multi-head
teacher_forcing_step: 0.01
min_teacher_forcing_ratio: 0.9
extractor: vgg
activation: hardtanh
cross_entropy_weight: 0.7
ctc_weight: 0.3
mask_conv: True
joint_ctc_attention: True
max_len: 400